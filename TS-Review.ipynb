{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating and Modeling Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Up\n",
    "\n",
    "Airline Passenger Data: https://www.kaggle.com/rakannimer/air-passengers\n",
    "\n",
    "(it's a pretty common dataset, available in several different places, but here's a source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "import pmdarima as pm\n",
    "from pmdarima import model_selection\n",
    "from pmdarima.utils import decomposed_plot\n",
    "from pmdarima.arima import decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to install pmdarima (plus a link to the documentation)\n",
    "# http://alkaline-ml.com/pmdarima/\n",
    "# !pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read df\n",
    "air_df = pd.read_csv('data/airline_passengers.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the data\n",
    "air_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the first 10 rows...\n",
    "air_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go ahead and set the index to be a datetime index\n",
    "# First making it a datetime object\n",
    "air_df['Month'] = pd.to_datetime(air_df['Month'])\n",
    "\n",
    "# Now making it our index\n",
    "air_df.set_index('Month', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now let's look at how that impacted the time/index\n",
    "air_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's get a sense of the data\n",
    "plt.figure(figsize=(16,6))\n",
    "air_df['Thousands of Passengers'].plot()\n",
    "plt.title('Thousands of Passengers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the general trends in our data.\n",
    "\n",
    "Also! This is why we change our data to use the datetime object as the index - makes EVERYTHING easier!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the frequency of our data \n",
    "\n",
    "Also called downsampling or upsampling, depending on whether you're going to a less frequent or more frequent point in time.\n",
    "\n",
    "[Here](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling) is a reference for resampling based on time frequency. (you can find the actual codes you can use as arguments in the resample function [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling to a daily cadence\n",
    "df_daily = air_df.resample('D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ew - nulls!\n",
    "df_daily.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we have nulls though - we took monthly data and tried to make it daily!\n",
    "\n",
    "But what about downsampling to quarterly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we're downsampling to quarter\n",
    "# Note that there are a few different ways to define 'quarter'\n",
    "df_quarterly = air_df.resample('Q').mean()\n",
    "df_quarterly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, visualizing the Average Opening price\n",
    "plt.figure(figsize=(16,6))\n",
    "df_quarterly['Thousands of Passengers'].plot()\n",
    "plt.title('Thousands of Passengers - Quarterly')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as would make sense, as you change the frequency of your data it changes the granularity (level of detail) that's conveyed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity\n",
    "\n",
    "Introduction to stationarity from [_Forecasting: Principles and Practice_](https://otexts.com/fpp2/stationarity.html):\n",
    "\n",
    "> \"A stationary time series is one whose properties do not depend on the time at which the series is observed.14 Thus, time series with trends, or with seasonality, are not stationary — the trend and seasonality will affect the value of the time series at different times. On the other hand, a white noise series is stationary — it does not matter when you observe it, it should look much the same at any point in time.\n",
    ">\n",
    "> \"Some cases can be confusing — a time series with cyclic behaviour (but with no trend or seasonality) is stationary. This is because the cycles are not of a fixed length, so before we observe the series we cannot be sure where the peaks and troughs of the cycles will be.\n",
    ">\n",
    "> \"In general, a stationary time series will have no predictable patterns in the long-term. Time plots will show the series to be roughly horizontal (although some cyclic behaviour is possible), with constant variance.\"\n",
    "\n",
    "And here's a [useful blog post](https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322) on the subject, where I found the below demonstrative image:\n",
    "\n",
    "![Examples of stationary and non-stationary processes, from the above medium blog](https://miro.medium.com/max/1400/1*tkx0_wwQ2JT7pSlTeg4yzg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we want to get a stationary series?\n",
    "\n",
    "> \"Stationarity means that the statistical properties of a time series (or rather the process generating it) do not change over time.\"\n",
    "\n",
    "- [Source](https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322)\n",
    "\n",
    "Biggest reason: makes the data easier to model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's  get the rolling mean and rolling standard deviation, using a 12-month window\n",
    "\n",
    "roll_mean = air_df['Thousands of Passengers'].rolling(window=12, center=False).mean()\n",
    "roll_std = air_df['Thousands of Passengers'].rolling(window=12, center=False).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "ax.plot(air_df['Thousands of Passengers'], color='blue',\n",
    "        label='Average Monthly Number of Passengers (Thousands)')\n",
    "\n",
    "ax.plot(roll_mean, color='red', label='Rolling 3-Month Mean')\n",
    "\n",
    "ax.plot(roll_mean + roll_std, color='black', linestyle='dotted')\n",
    "ax.plot(roll_mean - roll_std, color='black', linestyle='dotted')\n",
    "\n",
    "ax.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think this data is stationary? Why or why not?\n",
    "\n",
    " - \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a test for this!\n",
    "\n",
    "> **Augumented Dickey-Fuller test**: a hypothesis test, where we reject the null hypothesis (that a time series is non-stationary) if the test-statistic is less than the critical value\n",
    "\n",
    "[Documentation](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html) for the Dickey-Fuller test in StatsModels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's write out our null and alternative hypotheses (remember these??):\n",
    "\n",
    "Ho = \n",
    "\n",
    "Ha = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed in our column, since the test function expects a series:\n",
    "adfuller(air_df['Thousands of Passengers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's interpret the output of this test:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've determined whether the data is stationary, let's decompose it\n",
    "\n",
    "# Using the decompose function from pmdarima\n",
    "# Need to feed it an array, hence the .values attribute\n",
    "decomposed = decompose(air_df['Thousands of Passengers'].values, 'multiplicative', m=12)\n",
    "\n",
    "# M? See this: https://alkaline-ml.com/pmdarima/tips_and_tricks.html#period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the pieces using the arima model again\n",
    "decomposed_plot(decomposed, figure_kwargs={'figsize': (16, 10)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the documentation for `pmdarima`'s decompose:: \n",
    "\n",
    "> So what is happening when we call `decomposed`?\n",
    "> 1) The trend is extracted from the signal via a convolution using either a\n",
    "   SMA or a user-defined filter.\n",
    ">   \n",
    "> 2) We remove the effects of the trend from the original signal by either\n",
    "   subtracting its effects or dividing out its effects for 'additive' or\n",
    "   'multiplicative' types of decompositions, respectively. We then take the\n",
    "   mean across all seasons to get the values for a single season. For m=4, we\n",
    "   expect 4 values for a single season.\n",
    ">\n",
    "> 3) We then create the seasonal series by replicating the single season\n",
    "   until it is the same length of the trend signal.\n",
    ">\n",
    "> 4) Lastly to get the random/noise elements of the signal we remove the effects\n",
    "   of both the trend and seasonal series and we are now left with the\n",
    "   variation of the original signal that is neither explainable by seasonal\n",
    "   nor trend effects.\n",
    ">\n",
    "> This logic produces a named tuple of the original signal, trend, seasonal, and random components. It is this named tuple that is passed to `decomposed_plot`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additive vs Multiplicative?\n",
    "\n",
    "It's in the trends of the plots: \n",
    "\n",
    "| Additive Example | Multiplicative Example |\n",
    "| ---------------- | ---------------------- |\n",
    "| ![from the pmdarima documentation: additive example](images/sphx_glr_example_seasonal_decomposition_001.png) | ![from the pmdarima documentation: multiplicative example](images/sphx_glr_example_seasonal_decomposition_002.png) |\n",
    "\n",
    "[Source](https://alkaline-ml.com/pmdarima/auto_examples/arima/example_seasonal_decomposition.html)\n",
    "\n",
    "Can you spot the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now - Time to Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataset who dis\n",
    "# Monthly Google search trends for 'taxes' in the US\n",
    "df_taxes = pd.read_csv('data/google-trends_taxes_us.csv')\n",
    "\n",
    "# Some quick clean up\n",
    "df_taxes.columns = ['counts']\n",
    "df_taxes = df_taxes.iloc[1:]\n",
    "df_taxes['counts'] = df_taxes['counts'].str.replace('<1', '0').astype(int)\n",
    "df_taxes.index = pd.to_datetime(df_taxes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxes.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_taxes.plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we check out the index, you see freq=None\n",
    "df_taxes.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set that freq to be MS (month start)\n",
    "df_taxes.index.freq = 'MS'\n",
    "df_taxes.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's decompose this one\n",
    "\n",
    "decomposed = decompose(df_taxes['counts'].values, 'additive', m=12)\n",
    "\n",
    "decomposed_plot(decomposed, figure_kwargs={'figsize': (16, 10)})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARMA Modeling \n",
    "\n",
    "Once you determine if your time series is stationary, you can model. There are 4 key steps: \n",
    "1. Model Identification - where you determine the properties of a time series then chose a structural form. Remember you're treating the data as a series of random variables. The basic types of ARIMA models are: \n",
    "    - AutoRegressive(AR)\n",
    "    - Moving Average(MA) \n",
    "    - AutoRegressive Moving Average(ARMA)\n",
    "    - AutoRegressive Integrated Moving Average(ARIMA)\n",
    "\n",
    "A time series may be primarily an autoregressive, moving average or combination of both. To identify which it is, you need to plot 2 key functions.\n",
    "\n",
    "    > Sample Autocorrelation Function(ACF) \n",
    "    > Sample Partial Autocorrelation Function(Partial ACF)\n",
    "    \n",
    "2. Parameter Estimation - Once you have identified the form of an ARIMA model, the next step is to estimate the coefficients or parameters of the model. You can use Regression and MLE to do this. \n",
    "\n",
    "3. Model Checking - The most widely used information criterion(checking the quality of your model) for time series is AIC. You can compare different models with different numbers of lagged terms, white noise terms and how many times the time series was differenced and choose the model with the lowest AIC. \n",
    "\n",
    "4. Forecasting - Once the model is estimated you can forecast future values with the predict function. \n",
    "\n",
    "![](https://www.statisticshowto.com/wp-content/uploads/2016/11/lag-plot-linear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation and Partial Autocorrelation\n",
    "The ACF shows the correlations between the elements of a time series as a function of their lags. The partial ACF shows the correlations between the elements of a time series for each lag, holding constant the impact of all other lags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation Function Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"The **autocorrelation function** is a function that represents autocorrelation of a time series as a function of the time lag.\"\n",
    "\n",
    "The autocorrelation function tells interesting stories about trends and seasonality. For example, if the original time series repeats itself every five days, you would expect to see a spike in the autocorrelation function at 5 days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxes['counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxes['lag 1'] = df_taxes['counts'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxes['lag 12'] = df_taxes['counts'].shift(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taxes.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T17:23:35.743867Z",
     "start_time": "2020-10-16T17:23:35.386843Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.graphics.tsaplots as tsa\n",
    "\n",
    "tsa.plot_acf(df_taxes['counts'],lags=52);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T18:41:36.798858Z",
     "start_time": "2020-12-10T18:41:36.649776Z"
    }
   },
   "outputs": [],
   "source": [
    "# Another view\n",
    "plt.figure(figsize=(20, 4))\n",
    "pd.plotting.autocorrelation_plot(df_taxes['counts']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The horizontal bands represent condfidence intervals, which are calculated by taking relevant z-scores of the standard normal distribution and dividing by the square root of the number of observations. What do these intervals mean? - anything outside confidence interval means not due to chance - reject null. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial-Autocorrelation Function Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> \"The **partial autocorrelation function** can be interpreted as a regression of the series against its past lags.\n",
    " \n",
    " > It helps you come up with a possible order for the auto regressive term. The terms can be interpreted the same way as a standard linear regression, that is the contribution of a change in that particular lag while holding others constant. \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind Partial Autocorrelation is to compare a series to a lagged version of itself while abstracting away from intermediate values. In effect, this amounts to exploring the correlations among residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-16T17:24:17.186767Z",
     "start_time": "2020-10-16T17:24:16.893580Z"
    }
   },
   "outputs": [],
   "source": [
    "tsa.plot_pacf(df_taxes['counts'],lags=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA MODELS:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ARIMA Time Series Model\n",
    "\n",
    "One of the most common methods used in time series forecasting is known as the ARIMA model, which stands for **AutoregRessive Integrated Moving Average**. ARIMA is a model that can be fitted to time series data in order to better understand or predict future points in the series.\n",
    "\n",
    "Let's have a quick introduction to ARIMA. The ARIMA forecasting for a stationary time series is nothing but a linear (like a linear regression) equation. The predictors depend on the parameters (p,d,q) of the ARIMA model:\n",
    "\n",
    "### Number of AR (Auto-Regressive) terms (p): \n",
    "\n",
    "`p` is the auto-regressive part of the model. It allows us to incorporate the effect of past values into our model. Intuitively, this would be similar to stating that it is likely to rain tomorrow if it has been raining for past 3 days. AR terms are just lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5).\n",
    "\n",
    "### Number of Differences (d):\n",
    "\n",
    "`d` is the **Integrated** component of an ARIMA model. This value is concerned with the amount of differencing as it identifies the number of lag values to subtract from the current observation. Intuitively, this would be similar to stating that it is likely to rain tomorrow if the difference in amount of rain in the last *n* days is small. \n",
    "\n",
    "### Number of MA (Moving Average) terms (q): \n",
    "\n",
    "`q` is the moving average part of the model which is used to set the error of the model as a linear combination of the error values observed at previous time points in the past. MA terms form lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where `e(i)` is the difference between the moving average at ith instant and actual value.\n",
    "\n",
    "These three distinct integer values, (p, d, q), are used to parametrize ARIMA models. Because of that, ARIMA models are denoted with the notation `ARIMA(p, d, q)`. Together these three parameters account for seasonality, trend, and noise in datasets:\n",
    "\n",
    "* `(p, d, q)` are the non-seasonal parameters described above.\n",
    "* `(P, D, Q)` follow the same definition but are applied to the seasonal component of the time series. \n",
    "* The term `s` is the periodicity of the time series (4 for quarterly periods, 12 for yearly periods, etc.).\n",
    "\n",
    "A detailed article on these parameters is available [HERE](https://www.quantstart.com/articles/Autoregressive-Integrated-Moving-Average-ARIMA-p-d-q-Models-for-Time-Series-Analysis).\n",
    "\n",
    "The seasonal ARIMA method can appear daunting because of the multiple tuning parameters involved. In the next section, we will describe how to automate the process of identifying the optimal set of parameters for the seasonal ARIMA time series model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you generally will try to do for any time series analysis is:\n",
    "\n",
    "- Detrend your time series using differencing. ARMA models represent stationary processes, so we have to make sure there are no trends in our time series\n",
    "- Look at ACF and PACF of the time series\n",
    "- Decide on the AR, MA, and order of these models\n",
    "- Fit the model to get the correct parameters and use for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "# Note - we're back to regression metrics!\n",
    "def report_metrics(y_true, y_pred):\n",
    "    print(\"Explained Variance:\\n\\t\", metrics.explained_variance_score(y_true, y_pred))\n",
    "    print(\"MAE:\\n\\t\", metrics.mean_absolute_error(y_true, y_pred))\n",
    "    print(\"RMSE:\\n\\t\", metrics.mean_squared_error(y_true, y_pred, squared=False))\n",
    "    print(\"r^2:\\n\\t\", metrics.r2_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting rid of those earlier lag cols\n",
    "df_taxes = df_taxes.drop(columns=['lag 1', 'lag 12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to taxes\n",
    "# Let's create a time period tracker - incrementing each instance\n",
    "df_taxes.insert(0, 't', range(len(df_taxes)))\n",
    "df_taxes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's create a target area\n",
    "df_taxes['future'] = (df_taxes.index.year > 2017).astype('int')\n",
    "\n",
    "# Now plot\n",
    "plt.figure(figsize=(10,6))\n",
    "df_taxes.loc[df_taxes.future == 0, 'counts'].plot()\n",
    "df_taxes.loc[df_taxes.future == 1, 'counts'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, just using the instance as our \"X\" - not really what we're using to predict\n",
    "# But - this is setting us up for something later (linear predictions)\n",
    "X_train = df_taxes.loc[df_taxes.future == 0, 't'].values.reshape(-1, 1)\n",
    "X_test = df_taxes.loc[df_taxes.future == 1, 't'].values.reshape(-1, 1)\n",
    "# Our train set is our actual value in the series\n",
    "y_train = df_taxes.loc[df_taxes.future == 0, 'counts'].values\n",
    "y_test = df_taxes.loc[df_taxes.future == 1, 'counts'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What's the most naive way to make a prediction?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your naive predictions\n",
    "plt.figure(figsize=(10,6))\n",
    "df_taxes.loc[df_taxes.future == 0, 'counts'].plot()\n",
    "df_taxes.loc[df_taxes.future == 1, 'counts'].plot()\n",
    "\n",
    "# Predictions go here\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How right are we?\n",
    "report_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts? \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another Naive Approach - Linear Trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear trend approach\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "y_trend = lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(df_taxes['counts'])\n",
    "plt.plot(df_taxes.loc[df_taxes.future == 0].index, y_trend)\n",
    "plt.plot(df_taxes.loc[df_taxes.future == 1].index, y_pred, color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts? \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ARMA\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "\n",
    "# Fit an ARMA model - what should our parameters be?\n",
    "arima_order = None #should be a tuple, like (0,0,0)\n",
    "mod_arima = ARIMA(df_taxes.loc[df_taxes.future == 0]['counts'], \n",
    "                 order=arma_order)\n",
    "res_arima = mod_arma.fit()\n",
    "\n",
    "# Print out summary information on the fit\n",
    "print(res_arima.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, to make more predictions, how many steps do we want?\n",
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Returns 3 things - forecast, standard error, and confidence interval\n",
    "res_arima.forecast(steps = len(y_test))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving these predictions to more easily visualize\n",
    "future_df = df_taxes.loc[df_taxes.future == 1].copy()\n",
    "future_df['preds'] = res_arima.forecast(steps = len(y_test))[0]\n",
    "future_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize it!\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(df_taxes.loc[df_taxes.future == 0, 'counts'], color='blue', label='actual train')\n",
    "plt.plot(df_taxes.loc[df_taxes.future == 1, 'counts'], color='orange', label='actual test')\n",
    "plt.plot(future_df['preds'], color='green', label='predicted test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts? \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PMDArima - Using their Auto ARIMA! \n",
    "\n",
    "Basically, grid search for ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train test split - but now using PMDArima's function\n",
    "train, test = model_selection.train_test_split(df_taxes['counts'], test_size=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking stationarity\n",
    "from pmdarima.arima.stationarity import ADFTest\n",
    "\n",
    "# beyond statsmodels\n",
    "adf_test = ADFTest(alpha=0.05)\n",
    "p_val, should_diff = adf_test.should_diff(df_taxes['counts'])  # (0.01, False)\n",
    "\n",
    "print(f\"P-Value: {p_val}, so should you difference the data? {should_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to model!\n",
    "arima = pm.auto_arima(train, error_action='ignore', trace=True,\n",
    "                      suppress_warnings=True, maxiter=100,\n",
    "                      seasonal=True, m=12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the output summary\n",
    "arima.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = arima.predict(n_periods=test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot actual test vs. forecasts:\n",
    "x = np.arange(test.shape[0])\n",
    "plt.plot(x, test, color='orange', label='Actual')\n",
    "plt.plot(x, arima.predict(n_periods=test.shape[0]), color='green', label='Predicted')\n",
    "plt.title('Actual test samples vs. forecasts')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(df_taxes['counts'], label = 'actual')\n",
    "plt.plot(df_taxes.loc[df_taxes.future == 1].index, y_pred, color='green', label = 'predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More from the documentation: https://alkaline-ml.com/pmdarima/tips_and_tricks.html#understand-p-d-and-q\n",
    "\n",
    "> ARIMA models are made up of three different terms:\n",
    "> \n",
    "> p: The order of the auto-regressive (AR) model (i.e., the number of lag observations). A time series is considered AR when previous values in the time series are very predictive of later values. An AR process will show a very gradual decrease in the ACF plot.\n",
    "> \n",
    "> d: The degree of differencing.\n",
    "> \n",
    "> q: The order of the moving average (MA) model. This is essentially the size of the “window” function over your time series data. An MA process is a linear combination of past errors.\n",
    "\n",
    "OR:\n",
    "\n",
    "> Seasonal ARIMA models have three parameters that heavily resemble our p, d and q parameters:\n",
    "> \n",
    "> P: The order of the seasonal component for the auto-regressive (AR) model.\n",
    "> \n",
    "> D: The integration order of the seasonal process.\n",
    "> \n",
    "> Q: The order of the seasonal component of the moving average (MA) model.\n",
    "> \n",
    "> P and Q and be estimated similarly to p and q via auto_arima, and D can be estimated via a Canova-Hansen test, however m generally requires subject matter knowledge of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More gems at the end for those digging back into this notebook:\n",
    "\n",
    "`pmdarima` has a set of tips and tricks: https://alkaline-ml.com/pmdarima/tips_and_tricks.html\n",
    "\n",
    "Also:\n",
    "\n",
    "- https://towardsdatascience.com/time-series-forecasting-using-auto-arima-in-python-bb83e49210cd\n",
    "- https://machinelearningmastery.com/develop-arch-and-garch-models-for-time-series-forecasting-in-python/\n",
    "\n",
    "And, what I'm really looking into right now:\n",
    "- https://towardsdatascience.com/sktime-a-unified-python-library-for-time-series-machine-learning-3c103c139a55"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
